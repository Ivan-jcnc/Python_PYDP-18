{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
      "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
      "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
      "        1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
      "        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
      "        2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
      "        8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
      "        7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
      "        0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
      "        0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
      "        8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for i in train:\n",
    "    print(i[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss() \n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(): # функция обучения модели\n",
    "    for ep in range(num_epochs):\n",
    "        train_iters, train_passed  = 0, 0 # фиксируем характеристики обучения\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        \n",
    "        model.train()\n",
    "        for X, y in train: # итерируемся по тренировочной части датасета\n",
    "            trainer.zero_grad() # сбрасываем градиенты нашей модели\n",
    "            y_pred = model(X) # получаем предсказание\n",
    "            l = loss(y_pred, y) # сравниваем с помощью функции потерь\n",
    "            l.backward() # считаем градиенты\n",
    "            trainer.step() # делаем шаг\n",
    "            train_loss += l.item() # фиксируем количество элементов, которые мы прогнали\n",
    "            train_acc += (y_pred.argmax(dim=1) == y).sum().item() # количество правильно предсказанных элементов\n",
    "            \n",
    "            train_iters += 1 # добавляем количество итераций\n",
    "            train_passed += len(X) # смотрим сколько данных прогнали через нашу модель\n",
    "            \n",
    "        \n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        model.eval()\n",
    "        for X, y in test:\n",
    "            y_pred = model(X)\n",
    "            l = loss(y_pred, y)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += len(X)\n",
    "            \n",
    "        print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "            ep, train_loss / train_iters, train_acc / train_passed,\n",
    "            test_loss / test_iters, test_acc / test_passed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.22148384976180943, train_acc: 0.9336666666666666, test_loss: 0.12321777335600928, test_acc: 0.9624\n",
      "ep: 1, train_loss: 0.09412939180520938, train_acc: 0.97165, test_loss: 0.0950196890727966, test_acc: 0.9701\n",
      "ep: 2, train_loss: 0.05782979813780873, train_acc: 0.9829333333333333, test_loss: 0.09875793446190073, test_acc: 0.9692\n",
      "ep: 3, train_loss: 0.037989741112006474, train_acc: 0.9889, test_loss: 0.1010832858748472, test_acc: 0.9697\n",
      "ep: 4, train_loss: 0.026711100204154216, train_acc: 0.9919666666666667, test_loss: 0.10773993961147425, test_acc: 0.9708\n",
      "ep: 5, train_loss: 0.022753404461570637, train_acc: 0.99255, test_loss: 0.1250790519295606, test_acc: 0.9697\n",
      "ep: 6, train_loss: 0.019124501230482486, train_acc: 0.9938666666666667, test_loss: 0.11003976158199294, test_acc: 0.9747\n",
      "ep: 7, train_loss: 0.016650893367291923, train_acc: 0.9947833333333334, test_loss: 0.10147093100811162, test_acc: 0.9736\n",
      "ep: 8, train_loss: 0.012375930091861557, train_acc: 0.9960833333333333, test_loss: 0.10720092207025686, test_acc: 0.9738\n",
      "ep: 9, train_loss: 0.01188469895835411, train_acc: 0.99585, test_loss: 0.10043483763463427, test_acc: 0.9765\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
